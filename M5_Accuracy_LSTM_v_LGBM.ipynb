{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11189559,"sourceType":"datasetVersion","datasetId":6985344},{"sourceId":11189572,"sourceType":"datasetVersion","datasetId":6985351},{"sourceId":11189581,"sourceType":"datasetVersion","datasetId":6985357},{"sourceId":11189590,"sourceType":"datasetVersion","datasetId":6985364}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"2dc33e12-5f06-41b9-bf15-688a092b1705","_cell_guid":"d1a410de-c32e-4322-9827-3af96db194cf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:26.307472Z","iopub.execute_input":"2025-03-29T13:33:26.307750Z","iopub.status.idle":"2025-03-29T13:33:27.470040Z","shell.execute_reply.started":"2025-03-29T13:33:26.307712Z","shell.execute_reply":"2025-03-29T13:33:27.469155Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 1: Data Loading","metadata":{"_uuid":"3c386c8d-57c4-4bca-8cfe-454a80b8f2b2","_cell_guid":"25dd543c-cbec-46a8-aa14-feae934e7fbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n# Load datasets\ncalendar = pd.read_csv('/kaggle/input/calender/calendar.csv')\nsell_prices = pd.read_csv('/kaggle/input/sell-prices/sell_prices.csv')\nsales_train_eval = pd.read_csv('/kaggle/input/sales-train-eval/sales_train_evaluation.csv')\nsales_train_valid = pd.read_csv('/kaggle/input/sales-train-valid/sales_train_validation.csv')","metadata":{"_uuid":"d2526f86-decc-493c-b043-59ca1c646b88","_cell_guid":"70d97c79-5368-4da9-bf3d-52d48dac4c14","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:27.471076Z","iopub.execute_input":"2025-03-29T13:33:27.471707Z","iopub.status.idle":"2025-03-29T13:33:48.917025Z","shell.execute_reply.started":"2025-03-29T13:33:27.471668Z","shell.execute_reply":"2025-03-29T13:33:48.916246Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 2: Examining Datasets","metadata":{"_uuid":"8bd6249f-cffe-4597-a337-ebee6f30e8e4","_cell_guid":"11a38a9d-eb64-47fe-8116-c8ad7368f5d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Load the Calendar Dataset\n\nThe `calendar.csv` file contains daily information such as dates, event names, and the mapping from `d_1, d_2, ...` to actual dates.","metadata":{"_uuid":"5d04e93e-e62d-40fd-a0f7-dc119f3a1cfa","_cell_guid":"d6785ec4-4fb9-409c-99c8-75a27f05e79d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Calendar shape:\", calendar.shape)\n\n# Display the first 5 rows\ncalendar.head()","metadata":{"_uuid":"ad34583c-d53b-4e2f-897e-e6fca1463455","_cell_guid":"eb461e7a-7522-42e2-8c6a-5c389828728a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:48.917936Z","iopub.execute_input":"2025-03-29T13:33:48.918294Z","iopub.status.idle":"2025-03-29T13:33:48.952146Z","shell.execute_reply.started":"2025-03-29T13:33:48.918261Z","shell.execute_reply":"2025-03-29T13:33:48.951361Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-- Missing values in calendar --\")\nprint(calendar.isnull().sum())\n\nprint(\"\\n-- Data types (info) --\")\ncalendar.info()\n\nprint(\"\\n-- Descriptive statistics --\")\nprint(calendar.describe(include='all'))","metadata":{"_uuid":"5f9559ef-3a59-4c2a-9aa2-ad23a83268f2","_cell_guid":"4e2b4a57-dc21-445e-afd0-21b4a7ee748f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:48.954218Z","iopub.execute_input":"2025-03-29T13:33:48.954513Z","iopub.status.idle":"2025-03-29T13:33:49.017727Z","shell.execute_reply.started":"2025-03-29T13:33:48.954491Z","shell.execute_reply":"2025-03-29T13:33:49.016658Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load `sell_prices.csv`\n\nThis file contains information about the price of each item in each store, keyed by `wm_yr_wk`. We'll examine its shape and a few rows to understand how it aligns with the `calendar` and sales files.","metadata":{"_uuid":"3e14c285-bff5-470b-93b3-cfabee9dd40b","_cell_guid":"59092830-c5d3-47a2-9025-390925a5cd11","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Sell Prices shape:\", sell_prices.shape)\n\n# Display first 5 rows\nsell_prices.head()","metadata":{"_uuid":"5be6a875-fac0-44cd-a91c-0868abb9dd70","_cell_guid":"44107ac0-b9f9-4f07-86f0-9e6188c8dabe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:49.019094Z","iopub.execute_input":"2025-03-29T13:33:49.019425Z","iopub.status.idle":"2025-03-29T13:33:49.029323Z","shell.execute_reply.started":"2025-03-29T13:33:49.019403Z","shell.execute_reply":"2025-03-29T13:33:49.028262Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-- Missing values in sell_prices --\")\nprint(sell_prices.isnull().sum())\n\nprint(\"\\n-- Data types (info) --\")\nsell_prices.info()\n\nprint(\"\\n-- Descriptive stats for sell_price --\")\nprint(sell_prices[\"sell_price\"].describe())","metadata":{"_uuid":"4cbc192e-77a0-4683-8182-20beaab12e3a","_cell_guid":"61e73425-dc56-4f76-9125-2a1267b7bd60","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:49.030345Z","iopub.execute_input":"2025-03-29T13:33:49.030651Z","iopub.status.idle":"2025-03-29T13:33:49.886999Z","shell.execute_reply.started":"2025-03-29T13:33:49.030617Z","shell.execute_reply":"2025-03-29T13:33:49.886205Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the `sales_train_evaluation.csv`\n\nThis dataset holds the historical daily unit sales data per product and store from day 1 (d_1) to day 1941 (d_1941). It's a wide format with one column per day.","metadata":{"_uuid":"5f836a96-80f3-46f1-acd6-8f8bcfd38145","_cell_guid":"6398d562-0c9c-4f4f-857e-03981ee5b0db","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Sales Evaluation shape:\", sales_train_eval.shape)\n\n# Display first 5 rows\nsales_train_eval.head()","metadata":{"_uuid":"f6149b11-91af-436a-8334-e4633b2c53f2","_cell_guid":"e5a1fb86-2cb8-4978-83dd-5719161836de","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:49.887886Z","iopub.execute_input":"2025-03-29T13:33:49.888130Z","iopub.status.idle":"2025-03-29T13:33:49.907458Z","shell.execute_reply.started":"2025-03-29T13:33:49.888110Z","shell.execute_reply":"2025-03-29T13:33:49.906501Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-- Missing values in sales_eval --\")\nprint(sales_train_eval.isnull().sum().sum())  # sum of all missing across columns\n\nprint(\"\\n-- Data types (info) --\")\nsales_train_eval.info(verbose=False)  # 'verbose=False' to avoid printing all 1947 columns","metadata":{"_uuid":"3213d7d9-6ced-4b78-87a9-905ff6f5f596","_cell_guid":"a8c45f5b-f9cc-4c5d-96ef-83558bed529b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:49.908342Z","iopub.execute_input":"2025-03-29T13:33:49.908585Z","iopub.status.idle":"2025-03-29T13:33:50.048610Z","shell.execute_reply.started":"2025-03-29T13:33:49.908563Z","shell.execute_reply":"2025-03-29T13:33:50.047435Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Additional Stats on `sales_train_evaluation`\n\nBefore melting or merging, let's see how many unique items, stores, departments, categories, and states we have in this dataset.","metadata":{"_uuid":"5f503c09-fe10-466d-8451-25fd4da9648c","_cell_guid":"8d257b38-4550-4ee1-b1f1-07350e43163f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Unique item IDs:\", sales_train_eval[\"item_id\"].nunique())\nprint(\"Unique store IDs:\", sales_train_eval[\"store_id\"].nunique())\nprint(\"Unique dept IDs:\", sales_train_eval[\"dept_id\"].nunique())\nprint(\"Unique cat IDs:\", sales_train_eval[\"cat_id\"].nunique())\nprint(\"Unique state IDs:\", sales_train_eval[\"state_id\"].nunique())","metadata":{"_uuid":"7d09d933-1f6c-4b31-9b62-ec5eeda2c05e","_cell_guid":"2a3812f0-dc2c-45cf-bbc0-564d5392809c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:50.049665Z","iopub.execute_input":"2025-03-29T13:33:50.050108Z","iopub.status.idle":"2025-03-29T13:33:50.067424Z","shell.execute_reply.started":"2025-03-29T13:33:50.050061Z","shell.execute_reply":"2025-03-29T13:33:50.066369Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 3: Store-by-Store Merge\n\nTo avoid running out of memory, we'll process each `store_id` separately:\n\n1. Filter `sales_eval` for one store.\n2. Melt the subset so `d_1 ... d_1941` become a \"day\" column and a \"sales\" column.\n3. Merge with `calendar` on \"d\".\n4. Filter `sell_prices` for that store and merge on `[store_id, item_id, wm_yr_wk]`.\n5. Save the result as a separate `.pkl` file.\n6. Clear variables and move on to the next store.\n\nThis way we never hold the full ~60 million rows in memory at once.","metadata":{"_uuid":"3bf4d137-fd9a-4996-b5b4-1ea751931616","_cell_guid":"e5942710-16a2-4e29-bca7-d470f1907821","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import gc\nimport pandas as pd\nimport numpy as np\n\ndef reduce_memory_usage(df):\n    \"\"\" Reduce memory usage of a dataframe by downcasting numerical types where possible. \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type not in [object, 'category']:\n            c_min, c_max = df[col].min(), df[col].max()\n            if str(col_type).startswith('int'):\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                df[col] = df[col].astype(np.float32)\n        else:\n            if col_type == object and df[col].nunique() / len(df[col]) < 0.5:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"  Memory reduced: {start_mem:.2f} MB -> {end_mem:.2f} MB \"\n          f\"(Reduced by {(start_mem - end_mem)/start_mem*100:.1f}%)\")\n    return df\n\ndef process_store(store, sales_eval, calendar, sell_prices, output_dir):\n    \"\"\" Process data for a single store: filter, melt, merge, reduce memory, and save. \"\"\"\n    print(f\"\\nProcessing store: {store}\")\n\n    # Step 1: Filter sales data for the store\n    df_store = sales_eval[sales_eval[\"store_id\"] == store].copy()\n    df_store = reduce_memory_usage(df_store)\n\n    # Step 2: Melt the subset\n    fixed_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    date_cols = [c for c in df_store.columns if c.startswith(\"d_\")]\n    df_melted_sub = pd.melt(df_store, id_vars=fixed_cols, value_vars=date_cols, var_name=\"d\", value_name=\"sales\")\n    del df_store\n    gc.collect()\n\n    # Step 3: Merge with calendar\n    df_cal_sub = pd.merge(df_melted_sub, calendar, how=\"left\", on=\"d\")\n    del df_melted_sub\n    gc.collect()\n\n    # Step 4: Filter and merge sell prices\n    sp_sub = sell_prices[sell_prices[\"store_id\"] == store].copy()\n    sp_sub = reduce_memory_usage(sp_sub)\n    df_merged_sub = pd.merge(df_cal_sub, sp_sub, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])\n    del df_cal_sub, sp_sub\n    gc.collect()\n\n    # Step 5: Reduce memory usage again\n    df_merged_sub = reduce_memory_usage(df_merged_sub)\n\n    # Step 6: Save to disk\n    out_path = f\"{output_dir}/merged_{store}.pkl\"\n    df_merged_sub.to_pickle(out_path)\n    print(f\"  Saved merged data for store={store}, shape={df_merged_sub.shape} -> {out_path}\")\n\n    # Step 7: Clear memory\n    del df_merged_sub\n    gc.collect()\n\n# Make sure dataframes are loaded before calling this\noutput_directory = \"/kaggle/working\"\n\nall_stores = sales_train_eval[\"store_id\"].unique()\nprint(\"Found store_ids:\", all_stores)\n\nfor store in all_stores:\n    process_store(store, sales_train_eval, calendar, sell_prices, output_directory)","metadata":{"_uuid":"1913a212-ea06-42cb-b544-576824755371","_cell_guid":"171d631b-df27-49fb-9eb6-95a0c06ecf3e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-29T13:33:50.068467Z","iopub.execute_input":"2025-03-29T13:33:50.068801Z","iopub.status.idle":"2025-03-29T13:36:25.557275Z","shell.execute_reply.started":"2025-03-29T13:33:50.068766Z","shell.execute_reply":"2025-03-29T13:36:25.556212Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 4: Baseline Model (Naive)\n\n1. **Read** the wide `sales_train_evaluation.csv`, which has 30,490 rows (`_evaluation` IDs).  \n2. **Compute** the average sales for the last 28 days (`d_1914 ~ d_1941`) for each row, calling that `naive_mean`.  \n3. **Replicate** those rows as `_validation` by replacing `_evaluation` with `_validation` in the `id` string.  \n4. **Fill** F1..F28 for both `_validation` and `_evaluation` rows with the same `naive_mean`.  \n5. **Combine** them into one final `submission.csv` containing 60,980 rows (30,490 `_validation` + 30,490 `_evaluation`).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Load the sales_train_evaluation dataset\nsales_train_eval = pd.read_csv('/kaggle/input/sales-train-eval/sales_train_evaluation.csv')\n\n# Step 2: Calculate the average sales for the last 28 days (d_1914 to d_1941)\n# Identify the columns for the last 28 days (d_1914 ~ d_1941)\ndate_columns = [f\"d_{i}\" for i in range(1914, 1942)]\nsales_train_eval[\"naive_mean\"] = sales_train_eval[date_columns].mean(axis=1)\n\n# Step 3: Replicate rows as _validation and adjust the `id` column\n# Create a new validation dataframe by replacing '_evaluation' with '_validation' in the id\nsales_train_valid = sales_train_eval.copy()\nsales_train_valid['id'] = sales_train_valid['id'].str.replace('_evaluation', '_validation')\n\n# Step 4: Fill F1..F28 columns with the naive mean for both _validation and _evaluation\nfor i in range(1, 29):\n    sales_train_eval[f\"F{i}\"] = sales_train_eval[\"naive_mean\"]\n    sales_train_valid[f\"F{i}\"] = sales_train_valid[\"naive_mean\"]\n\n# Step 5: Combine the _evaluation and _validation data into a single DataFrame\nfinal_submission = pd.concat([sales_train_eval, sales_train_valid], axis=0)\n\n# Step 6: Save the final submission to a CSV file\nfinal_submission.to_csv('/kaggle/working/final_submission.csv', index=False)\nprint(\"Final submission saved!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.638Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 5: Improving Naive Submission (LightGBM)\nNow, we will load the merged data for a specific store (CA_1) to explore more advanced methods such as feature engineering. This step helps us understand the available columns and prepare for a better predictive model.","metadata":{}},{"cell_type":"code","source":"# Load merged data for store CA_1 \ndf_tx1 = pd.read_pickle(\"/kaggle/working/merged_TX_1.pkl\")\n\nprint(\"Shape of df_tx1:\", df_tx1.shape)\nprint(df_tx1.head(3))\n\n# Check the columns to see what information is available\nprint(\"\\nColumns in df_tx1:\", df_tx1.columns.tolist())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.640Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a Numeric Day Index and Sampling\n\nNow that we have loaded the merged data for TX_1, we can convert the \"d\" column (e.g. \"d_1\", \"d_2\") into a numeric day index. This helps us handle time-based operations more easily. We'll also take a small sample to explore the data without overwhelming memory or visuals.","metadata":{}},{"cell_type":"code","source":"# Convert \"d\" (like \"d_1\") into an integer day index, and sample a portion\ndf_tx1[\"d_num\"] = df_tx1[\"d\"].str[2:].astype(int)\n\nprint(\"Added 'd_num' column. First few values:\")\nprint(df_tx1[[\"d\", \"d_num\"]].head(5))\n\n# Optional: Take a small sample for further exploration\ndf_tx1_sample = df_tx1.sample(100_000, random_state=42)\n\nprint(\"\\nSampled 100,000 rows. Shape:\", df_tx1_sample.shape)\nprint(df_tx1_sample.head(3))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.640Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analysis of Sample of TX_1\n\nWe want to see average sales across the days. This will help get an initial sense of time-based trends without being overwhelmed with the huge dataset.","metadata":{}},{"cell_type":"code","source":"# Group by the \"date\" column to see average sales per day in the sample\navg_sales_by_date = (\n    df_tx1_sample\n    .groupby(\"date\", as_index=False)[\"sales\"]\n    .mean()\n    .rename(columns={\"sales\": \"avg_sales_sample\"})\n)\n\nprint(\"Shape of avg_sales_by_date:\", avg_sales_by_date.shape)\nprint(avg_sales_by_date.head(5))\n\n# (Optional) Sort by date if needed for a clearer chronological view\navg_sales_by_date = avg_sales_by_date.sort_values(\"date\")\nprint(\"\\nFirst 5 rows after sorting by date:\")\nprint(avg_sales_by_date.head(5))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Sales over Time of Sample TX_1","metadata":{}},{"cell_type":"code","source":"# Plot avg_sales_sample vs. date\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,4))\nsns.lineplot(data=avg_sales_by_date, x=\"date\", y=\"avg_sales_sample\")\nplt.title(\"Daily Average Sales (Sample of 100k rows)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Avg Sales in Sample\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extending Feature Engineering","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import ParameterGrid\nimport gc\n\n# List of all store identifiers for processing\nall_stores = [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\",\n              \"TX_1\", \"TX_2\", \"TX_3\",\n              \"WI_1\", \"WI_2\", \"WI_3\"]\n\n# Parameter grid for LightGBM hyperparameter optimization\nparam_grid = {\n    \"num_leaves\": [31, 63],        # Number of leaves in each tree\n    \"learning_rate\": [0.05, 0.1],  # Learning rate for the model\n    \"n_estimators\": [100, 200]     # Number of trees to fit\n}\n\n# Mapping for weekdays to numeric values\nweekday_map = {\n    \"Monday\": 1, \"Tuesday\": 2, \"Wednesday\": 3,\n    \"Thursday\": 4, \"Friday\": 5, \"Saturday\": 6, \"Sunday\": 7\n}\n\n# Dictionary to store metrics for each store\nstore_metrics = {}\n\n# Iterate over each store to process data and train model\nfor store in all_stores:\n    pickle_path = f\"/kaggle/working/merged_{store}.pkl\"  # Path to the data file\n    \n    # Check if the data file exists; if not, skip to the next store\n    if not os.path.exists(pickle_path):\n        print(f\"File not found for {store}, skipping.\")\n        continue\n\n    print(f\"\\nProcessing {store} ...\")\n    \n    # Load the merged data for the current store\n    df_merged = pd.read_pickle(pickle_path).copy()\n\n    # Convert specified columns to categorical data type to save memory\n    for col in [\"weekday\", \"month\", \"year\"]:\n        df_merged[col] = df_merged[col].astype(\"category\")\n    \n    # Convert 'd' column to a numeric format by extracting values\n    df_merged[\"d_num\"] = df_merged[\"d\"].str[2:].astype(int)\n    df_merged = df_merged.sort_values([\"id\", \"d_num\"])  # Sort data by id and date\n\n    # Convert weekday strings to numeric values using the weekday_map\n    if df_merged[\"weekday\"].dtype in [object, \"category\"]:\n        df_merged[\"weekday_num\"] = (\n            df_merged[\"weekday\"].astype(str)\n            .map(weekday_map)\n            .fillna(0)\n            .astype(int)\n        )\n    else:\n        # If already numeric, fill missing values with 0\n        df_merged[\"weekday_num\"] = df_merged[\"weekday\"].fillna(0).astype(int)\n\n    # Convert month and year to integers, ensuring no categorical data type\n    df_merged[\"month\"] = df_merged[\"month\"].astype(int).fillna(0).astype(int)\n    df_merged[\"year\"]  = df_merged[\"year\"].astype(int).fillna(0).astype(int)\n\n    # Fill missing sell_price values with 0\n    df_merged[\"sell_price\"] = df_merged[\"sell_price\"].fillna(0)\n\n    # Create lagged and rolling features for sales\n    lags = [7, 14, 28]          # List of lags for sales\n    rolling_windows = [7, 28]   # Rolling window sizes for sales\n    grouped = df_merged.groupby(\"id\", observed=False)  # Group data by store id\n    \n    # Generate lagged sales features\n    for lag in lags:\n        col_name = f\"sales_lag{lag}\"\n        df_merged[col_name] = grouped[\"sales\"].shift(lag)\n    \n    # Generate rolling mean features for sales\n    for w in rolling_windows:\n        col_name = f\"sales_rollmean{w}\"\n        df_merged[col_name] = (\n            grouped[\"sales\"].shift(1).rolling(w, min_periods=1).mean()\n        )\n    \n    # Fill missing values for snap features\n    df_merged[[\"snap_CA\",\"snap_TX\",\"snap_WI\"]] = df_merged[[\"snap_CA\",\"snap_TX\",\"snap_WI\"]].fillna(0)\n    \n    # Example of additional rolling price feature\n    df_merged[\"price_roll7\"] = grouped[\"sell_price\"].shift(1).rolling(7, min_periods=1).mean()\n    \n    # Prepare a list of feature columns for training\n    feature_cols = []\n    for lag in lags:\n        feature_cols.append(f\"sales_lag{lag}\")\n    for w in rolling_windows:\n        feature_cols.append(f\"sales_rollmean{w}\")\n    feature_cols.append(\"price_roll7\")\n\n    # Include additional features needed for the model\n    extra_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\", \"sell_price\", \"weekday_num\", \"month\", \"year\"]\n    feature_cols += extra_cols\n\n    # Fill any remaining missing values in feature columns with 0\n    df_merged[feature_cols] = df_merged[feature_cols].fillna(0)\n\n    # Split data into training and validation sets based on the time (d_num value)\n    df_train = df_merged[df_merged[\"d_num\"] < 1800].copy()  # Training data\n    df_val   = df_merged[df_merged[\"d_num\"] >= 1800].copy()  # Validation data\n\n    # Check if there's enough data for training and validation\n    if len(df_train) == 0 or len(df_val) == 0:\n        print(f\"No data for {store}, skipping training.\")\n        continue\n\n    # Define training and validation features and labels\n    X_train = df_train[feature_cols]\n    y_train = df_train[\"sales\"]\n    X_val   = df_val[feature_cols]\n    y_val   = df_val[\"sales\"]\n\n    best_rmse = float(\"inf\")  # Initialize best RMSE\n    best_params = None         # Initialize variable to store best parameters\n\n    # Train the model using GridSearch over the parameter grid\n    for params in ParameterGrid(param_grid):\n        model = LGBMRegressor(random_state=42, **params)  # Initialize model with parameters\n        model.fit(X_train, y_train)  # Fit model to training data\n\n        # Predict validation data\n        pred_val = model.predict(X_val)\n        rmse = np.sqrt(np.mean((pred_val - y_val)**2))  # Calculate RMSE for validation predictions\n\n        # Update best RMSE and parameters if current RMSE is lower\n        if rmse < best_rmse:\n            best_rmse = rmse\n            best_params = params\n\n    # Store the best parameters and RMSE for the current store\n    print(f\"{store} best params: {best_params}, RMSE: {best_rmse:.4f}\")\n    store_metrics[store] = (best_params, best_rmse)\n\n    # Clear memory for the current store's data to optimize memory usage\n    del df_merged, df_train, df_val, X_train, X_val, y_train, y_val\n    gc.collect()  # Force garbage collection\n\n# Print all store metrics at the end of processing\nprint(\"\\nAll store metrics:\")\nfor st, (pars, sc) in store_metrics.items():\n    print(f\"{st} -> RMSE: {sc:.4f}, best params: {pars}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Dropout\nimport tensorflow.keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Enable multi-GPU strategy for TensorFlow to improve performance during training\nstrategy = tf.distribute.MirroredStrategy()\n\n# Enable memory growth on available GPU devices to prevent allocation errors\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# List of all store identifiers to process\nall_stores = [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\",\n              \"TX_1\", \"TX_2\", \"TX_3\",\n              \"WI_1\", \"WI_2\", \"WI_3\"]\n\n# Create a CSV file to store LightGBM predictions\nlgbm_file = open(\"submission_lgbm.csv\", \"w\")\nlgbm_file.write(\"id,\" + \",\".join([f\"F{i}\" for i in range(1, 29)]) + \"\\n\")\n\n# Process each store's data\nfor store in all_stores:\n    # Load the pre-processed data for the current store\n    pickle_path = f\"/kaggle/working/merged_{store}.pkl\"\n    if not os.path.exists(pickle_path):\n        print(f\"No file for {store}, skipping.\")\n        continue  # Skip if the data file does not exist\n\n    print(f\"\\nProcessing {store}...\")\n    # Read the data into a DataFrame\n    df_merged = pd.read_pickle(pickle_path)\n    df_merged[\"d_num\"] = df_merged[\"d\"].str[2:].astype(int)  # Convert date to numerical format\n    df_merged = df_merged.sort_values([\"id\", \"d_num\"])  # Sort data by 'id' and 'd_num'\n    \n    # Load the best hyperparameters for the LightGBM model for this store\n    best_params = store_metrics[store][0]\n    \n    # Define feature columns by creating lagged and rolling features\n    lags = [7, 14, 28]  # List of lag periods for sales\n    rolling_windows = [7, 28]  # List of rolling windows for sales averages\n    \n    # Generate lagged sales features\n    for lag in lags:\n        df_merged[f\"sales_lag{lag}\"] = df_merged[\"sales\"].shift(lag)\n\n    # Generate rolling mean features for sales over specified windows\n    for window in rolling_windows:\n        df_merged[f\"sales_rollmean{window}\"] = df_merged[\"sales\"].rolling(window=window).mean()\n\n    # Compute additional price-related features\n    df_merged[\"price_roll7\"] = df_merged[\"sell_price\"].rolling(window=7).mean()\n\n    # Fill missing values in feature columns with 0 and convert to float32 type\n    feature_cols = [f\"sales_lag{x}\" for x in lags] + [f\"sales_rollmean{x}\" for x in rolling_windows]\n    feature_cols += [\"sell_price\", \"price_roll7\"]\n    df_merged[feature_cols] = df_merged[feature_cols].fillna(0).astype(np.float32)\n\n    # Prepare features and labels for training LightGBM\n    X_lgbm = df_merged[feature_cols].values  # Feature matrix for LightGBM\n    y_lgbm = df_merged[\"sales\"].values       # Target variable (sales)\n\n    # Train the LightGBM model using the specified hyperparameters\n    lgbm_model = LGBMRegressor(random_state=42, **best_params, device=\"gpu\", gpu_platform_id=0, gpu_device_id=0)\n    # Create a validation set by splitting data\n    X_train_lgbm, X_val_lgbm, y_train_lgbm, y_val_lgbm = train_test_split(X_lgbm, y_lgbm, test_size=0.1, random_state=42)\n\n    # Fit the model while tracking validation performance\n    lgbm_model.fit(X_train_lgbm, y_train_lgbm,\n                   eval_set=[(X_train_lgbm, y_train_lgbm), (X_val_lgbm, y_val_lgbm)],\n                   eval_metric=\"rmse\")\n\n    # Extract RMSE results from training\n    results = lgbm_model.evals_result_\n    train_rmse = results['training']['rmse']\n    valid_rmse = results['valid_1']['rmse']\n\n    # Plot RMSE curves for training and validation\n    plt.plot(train_rmse, label='Train RMSE')\n    plt.plot(valid_rmse, label='Validation RMSE')\n    plt.xlabel('Iteration')\n    plt.ylabel('RMSE')\n    plt.title(f'Training and Validation RMSE curves - Store {store}')\n    plt.legend()\n    plt.savefig(f\"LGBM_Training_Validation_Curve_{store}.png\", dpi=300)  # Save plot as a PNG file\n    plt.show()  # Display plot\n\n    # Predict future sales using the trained LightGBM model\n    future_days = np.arange(1942, 1970)  # Define future days for prediction\n    all_ids = df_merged[\"id\"].unique()  # Get unique store IDs\n    # Create a DataFrame for future predictions\n    future_df = pd.DataFrame({\"id\": np.repeat(all_ids, len(future_days)), \n                              \"d_num\": np.tile(future_days, len(all_ids)),\n                              \"sales\": np.nan})\n    \n    # Concatenate future data with the original dataset\n    df_merged = pd.concat([df_merged, future_df], ignore_index=True, sort=False)\n    df_merged = df_merged.sort_values([\"id\", \"d_num\"]).reset_index(drop=True)\n\n    # Prepare feature data for future predictions\n    future_features = df_merged[feature_cols].iloc[-len(future_df):].values\n    \n    # Generate predictions for future sales\n    predictions_lgbm = lgbm_model.predict(future_features)\n    \n    # Save the predictions to the CSV file\n    for i, pred in enumerate(predictions_lgbm):\n        lgbm_file.write(f\"{future_df['id'].iloc[i]},{','.join([str(pred) for _ in range(28)])}\\n\")\n\n    # Clean up memory by deleting temporary variables\n    del df_merged, X_lgbm, y_lgbm, predictions_lgbm, future_features\n    gc.collect()  # Force garbage collection\n\n# Close the LightGBM results CSV file after writing all predictions\nlgbm_file.close()\nprint(\"LightGBM results saved to submission_lgbm.csv\")  # Confirmation of saved results","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-29T14:15:56.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install keras-tuner","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport keras_tuner as kt\nimport numpy as np\nimport pandas as pd\nimport gc\nimport os\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# RMSSE custom metric definition\ndef rmsse_wrapper(training_data):\n    \"\"\"\n    Wrapper function to compute Root Mean Squared Scaled Error (RMSSE) as a custom metric.\n    The metric is based on the true values and model predictions normalized by the training data.\n    \"\"\"\n    training_data = tf.convert_to_tensor(training_data, dtype=tf.float32)  # Convert training data to tensor for calculations\n    training_mean = tf.reduce_mean(training_data)  # Compute mean of the training data\n\n    def rmsse(y_true, y_pred):\n        \"\"\"\n        Computes RMSSE by normalizing RMSE with the mean squared differences in training data.\n        Args:\n            y_true: The true sales values.\n            y_pred: The predicted sales values by the model.\n\n        Returns:\n            The scaled error as a tensor.\n        \"\"\"\n        # Calculate the scaled root mean squared error\n        scaled_error = tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred))) / tf.sqrt(tf.reduce_mean(tf.square(training_data - training_mean)))\n        return scaled_error\n\n    return rmsse  # Return the RMSSE computation function\n\n\n# Function to create LSTM-compatible data\ndef create_lstm_data(df, feature_cols, seq_length=28):\n    \"\"\"\n    Prepares input sequences for LSTM model training.\n    Args:\n        df: The DataFrame containing the data.\n        feature_cols: List of feature columns to use for the model.\n        seq_length: The number of time steps to include in each input sequence.\n\n    Returns:\n        Tuple containing:\n        - Prepared input sequences for LSTM\n        - Scaled outputs (sales values)\n        - Scaler for features\n        - Scaler for targets (sales)\n    \"\"\"\n    scaler_x = StandardScaler()  # Scaler for feature normalization\n    scaler_y = StandardScaler()  # Scaler for target normalization\n    scaled_features = scaler_x.fit_transform(df[feature_cols].astype(np.float32))  # Scale features to standard Gaussian\n\n    X, y = [], []\n    for i in range(seq_length, len(df)):\n        # Create sequences of features for LSTM input\n        X.append(scaled_features[i-seq_length:i])\n        y.append(df[\"sales\"].values[i])  # Append corresponding output (sales) for each sequence\n\n    y = np.array(y, dtype=np.float32).reshape(-1, 1)  # Reshape target values for scaling\n    y_scaled = scaler_y.fit_transform(y).flatten()  # Scale target values\n\n    return np.array(X, dtype=np.float32), y_scaled, scaler_x, scaler_y  # Return feature and target data\n\n\ndef build_lstm_model_with_hp(hp, input_shape, training_data):\n    \"\"\"\n    Builds an LSTM model with hyperparameter tuning using Keras Tuner.\n    Args:\n        hp: Hyperparameter object for tuning.\n        input_shape: Shape of the input data.\n        training_data: The training data used for RMSSE calculation.\n\n    Returns:\n        Compiled LSTM model with specified hyperparameters.\n    \"\"\"\n    model = Sequential()  # Initialize sequential model\n\n    # Define the number of LSTM units with hyperparameter tuning\n    units = hp.Int('units', min_value=32, max_value=128, step=32)\n\n    # First input LSTM layer\n    model.add(LSTM(units, return_sequences=True, input_shape=input_shape))\n\n    # Hyperparameter search for additional LSTM layers\n    num_lstm_layers = hp.Int('num_lstm_layers', min_value=1, max_value=3, step=1)\n    \n    for _ in range(num_lstm_layers - 1):  # Add additional LSTM layers if needed\n        model.add(LSTM(units, return_sequences=True))\n\n    # Final LSTM layer\n    model.add(LSTM(units, return_sequences=False))\n\n    # Fully connected Dense layers\n    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))  # Hidden layer\n    model.add(Dense(1))  # Output layer for predictions\n\n    # Hyperparameter search for learning rate and optimizer\n    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG')\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Adam optimizer with variable learning rate\n\n    # Compile the model with mean squared error loss and RMSSE as the metric\n    model.compile(loss='mse', optimizer=optimizer, metrics=[rmsse_wrapper(training_data)])\n    \n    return model  # Return the constructed and compiled model\n    \n\ndef tune_lstm_model(X_train_lstm, y_train_lstm, input_shape, training_data):\n    \"\"\"\n    Uses Keras Tuner to search for the best hyperparameters for the LSTM model.\n    Args:\n        X_train_lstm: Training feature data for LSTM.\n        y_train_lstm: Training target data (sales).\n        input_shape: Shape of the input data.\n        training_data: The raw training data used for RMSSE normalization.\n\n    Returns:\n        The best model found during hyperparameter tuning.\n    \"\"\"\n    tuner = kt.Hyperband(\n        lambda hp: build_lstm_model_with_hp(hp, input_shape, training_data),  # Tuner to build models with specific hyperparameters\n        objective='val_loss',  # Objective metric to minimize\n        max_epochs=20,  # Maximum number of epochs for training\n        factor=3,  # Proportion of trials to keep for the next round\n        directory='lstm_tuning',  # Directory to save tuning results\n        project_name='lstm_search',  # Project name for organizing tunings\n        overwrite=True  # Overwrite existing results\n    )\n\n    # Run hyperparameter tuning on the training data\n    tuner.search(X_train_lstm, y_train_lstm, epochs=20, validation_split=0.1)\n    best_hps = tuner.get_best_hyperparameters(num_trials=5)[0]  # Get the best hyperparameters\n    print(\"Best Hyperparameters:\", best_hps.values)  # Log the best hyperparameters\n\n    # Build and return the model using the best hyperparameters\n    best_model = build_lstm_model_with_hp(best_hps, input_shape, training_data)\n    \n    return best_model\n\n\n# Open file to store predictions for later submission\nlstm_file = open(\"submission_lstm.csv\", \"w\")\nlstm_file.write(\"id,\" + \",\".join([f\"F{i}\" for i in range(1, 29)]) + \"\\n\")  # Write CSV header with forecast indicators\n\n# Process each store's data in the list of stores\nfor store in all_stores:\n    pickle_path = f\"/kaggle/working/merged_{store}.pkl\"  # Path to store-specific data file\n    if not os.path.exists(pickle_path):  # Check if the data file exists\n        print(f\"No file for {store}, skipping.\")\n        continue  # Skip if not found\n\n    print(f\"\\nProcessing {store}...\")\n    df_merged = pd.read_pickle(pickle_path)  # Load data into DataFrame\n    df_merged[\"d_num\"] = df_merged[\"d\"].str[2:].astype(int)  # Convert date column to numeric format\n    df_merged = df_merged.sort_values([\"id\", \"d_num\"])  # Sort values by store ID and date\n    \n    # Feature engineering: Create lagged sales and rolling mean features\n    lags = [7, 14, 28]  # Lag periods for previous sales\n    rolling_windows = [7, 28]  # Rolling mean computation windows\n    feature_cols = [f\"sales_lag{x}\" for x in lags] + [f\"sales_rollmean{x}\" for x in rolling_windows]\n    feature_cols += [\"sell_price\", \"price_roll7\"]\n    \n    # Create lagged and rolling features\n    for lag in lags:\n        df_merged[f\"sales_lag{lag}\"] = df_merged[\"sales\"].shift(lag)\n    for window in rolling_windows:\n        df_merged[f\"sales_rollmean{window}\"] = df_merged[\"sales\"].rolling(window=window).mean()\n    df_merged[\"price_roll7\"] = df_merged[\"sell_price\"].rolling(window=7).mean()\n    \n    # Handle missing values in feature columns by filling with column mean\n    df_merged[feature_cols] = df_merged[feature_cols].fillna(df_merged[feature_cols].mean())\n\n    # Prepare and scale LSTM training data\n    X_train_lstm, y_train_lstm, scaler_x, scaler_y = create_lstm_data(df_merged, feature_cols)\n    \n    # Tune LSTM model to find best hyperparameters\n    best_lstm_model = tune_lstm_model(X_train_lstm, y_train_lstm, (X_train_lstm.shape[1], X_train_lstm.shape[2]), df_merged[\"sales\"].values)\n    \n    # Train the best model found during hyperparameter tuning\n    history = best_lstm_model.fit(X_train_lstm, y_train_lstm, epochs=20, verbose=1, validation_split=0.1)\n    \n    # Plot and visualize training and validation RMSSE for each epoch\n    plt.figure(figsize=(12, 6))\n    plt.plot(history.history['rmsse'], label='Training RMSSE')\n    plt.plot(history.history['val_rmsse'], label='Validation RMSSE')\n    plt.title(f\"Training and Validation RMSSE for {store}\")  # Title for the plot\n    plt.xlabel('Epochs')  # X-axis label\n    plt.ylabel('RMSSE')  # Y-axis label\n    plt.legend()  # Show legend\n    plt.grid(True)  # Add grid\n    plt.savefig(f'RMSSE_curve_{store}.png')  # Save plot as an image\n    plt.close()  # Close the plot to free memory\n    \n    # Prepare for future sales predictions using the LSTM model\n    future_lstm_data = np.array([X_train_lstm[-1]])  # Start predictions from the last known input sequence\n    predictions_lstm = []  # List to store future predictions\n    for t in range(28):  # Generate predictions for the next 28 time steps\n        pred = best_lstm_model.predict(future_lstm_data)[0][0]  # Predict the next sales value\n        predictions_lstm.append(pred)  # Store the prediction\n        # Roll the future data for the next prediction\n        future_lstm_data = np.roll(future_lstm_data, shift=-1, axis=1)\n        future_lstm_data[0, -1, 0] = pred  # Update the last feature with the predicted sales value\n    \n    # Inverse scale the predictions to retrieve original sales values\n    predictions_lstm = scaler_y.inverse_transform(np.array(predictions_lstm).reshape(-1, 1)).flatten()\n    \n    # Save predictions to the CSV file for submission\n    for i, pred in enumerate(predictions_lstm):\n        lstm_file.write(f\"{df_merged['id'].iloc[i]},{','.join([str(pred) for _ in range(28)])}\\n\")  # Write predictions for future time steps\n\n    # Clear memory of the DataFrame and arrays after processing each store\n    del df_merged, X_train_lstm, y_train_lstm, future_lstm_data, predictions_lstm\n    gc.collect()  # Perform garbage collection to free memory\n\nlstm_file.close()  # Close the predictions file\nprint(\"LSTM results saved to submission_lstm.csv\")  # Confirmation message after saving results","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}