{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11189559,"sourceType":"datasetVersion","datasetId":6985344},{"sourceId":11189572,"sourceType":"datasetVersion","datasetId":6985351},{"sourceId":11189581,"sourceType":"datasetVersion","datasetId":6985357},{"sourceId":11189590,"sourceType":"datasetVersion","datasetId":6985364}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n# Load datasets\ncalendar = pd.read_csv('/kaggle/input/calender/calendar.csv')\nsell_prices = pd.read_csv('/kaggle/input/sell-prices/sell_prices.csv')\nsales_train_eval = pd.read_csv('/kaggle/input/sales-train-eval/sales_train_evaluation.csv')\nsales_train_valid = pd.read_csv('/kaggle/input/sales-train-valid/sales_train_validation.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Store-by-Store Merge","metadata":{}},{"cell_type":"code","source":"import gc\nimport pandas as pd\nimport numpy as np\n\ndef reduce_memory_usage(df):\n    \"\"\" Reduce memory usage of a dataframe by downcasting numerical types where possible. \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type not in [object, 'category']:\n            c_min, c_max = df[col].min(), df[col].max()\n            if str(col_type).startswith('int'):\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                df[col] = df[col].astype(np.float32)\n        else:\n            if col_type == object and df[col].nunique() / len(df[col]) < 0.5:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"  Memory reduced: {start_mem:.2f} MB -> {end_mem:.2f} MB \"\n          f\"(Reduced by {(start_mem - end_mem)/start_mem*100:.1f}%)\")\n    return df\n\ndef process_store(store, sales_eval, calendar, sell_prices, output_dir):\n    \"\"\" Process data for a single store: filter, melt, merge, reduce memory, and save. \"\"\"\n    print(f\"\\nProcessing store: {store}\")\n\n    # Step 1: Filter sales data for the store\n    df_store = sales_eval[sales_eval[\"store_id\"] == store].copy()\n    df_store = reduce_memory_usage(df_store)\n\n    # Step 2: Melt the subset\n    fixed_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    date_cols = [c for c in df_store.columns if c.startswith(\"d_\")]\n    df_melted_sub = pd.melt(df_store, id_vars=fixed_cols, value_vars=date_cols, var_name=\"d\", value_name=\"sales\")\n    del df_store\n    gc.collect()\n\n    # Step 3: Merge with calendar\n    df_cal_sub = pd.merge(df_melted_sub, calendar, how=\"left\", on=\"d\")\n    del df_melted_sub\n    gc.collect()\n\n    # Step 4: Filter and merge sell prices\n    sp_sub = sell_prices[sell_prices[\"store_id\"] == store].copy()\n    sp_sub = reduce_memory_usage(sp_sub)\n    df_merged_sub = pd.merge(df_cal_sub, sp_sub, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])\n    del df_cal_sub, sp_sub\n    gc.collect()\n\n    # Step 5: Reduce memory usage again\n    df_merged_sub = reduce_memory_usage(df_merged_sub)\n\n    # Step 6: Save to disk\n    out_path = f\"{output_dir}/merged_{store}.pkl\"\n    df_merged_sub.to_pickle(out_path)\n    print(f\"  Saved merged data for store={store}, shape={df_merged_sub.shape} -> {out_path}\")\n\n    # Step 7: Clear memory\n    del df_merged_sub\n    gc.collect()\n\n# Make sure dataframes are loaded before calling this\noutput_directory = \"/kaggle/working\"\n\nall_stores = sales_train_eval[\"store_id\"].unique()\nprint(\"Found store_ids:\", all_stores)\n\nfor store in all_stores:\n    process_store(store, sales_train_eval, calendar, sell_prices, output_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n# Loop over each store dataset\nfor store in all_stores:\n    pickle_path = f\"/kaggle/working/merged_{store}.pkl\"\n    \n    if not os.path.exists(pickle_path):\n        print(f\"No file for {store}, skipping.\")\n        continue\n\n    print(f\"\\nProcessing {store}...\")\n    df_merged = pd.read_pickle(pickle_path)\n\n    # Sample the data for all visualizations (10% of the data)\n    df_sampled = df_merged.sample(frac=0.1, random_state=42)\n    \n    # Basic info about the dataframe\n    print(df_sampled.info())\n    \n    \n    # Check the first few rows to understand the structure\n    print(df_sampled.head())\n    # Check missing values\n    missing_values = df_sampled.isnull().sum()\n    print(\"Missing values per column:\\n\", missing_values)\n\n    # visualize missing values\n\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(df_sampled.isnull(), cbar=False, cmap='viridis')\n    plt.title(f\"Missing Values Heatmap for {store}\")\n    plt.savefig(f'missing_vals_heatmap_{store}.png', dpi=300)\n    plt.show()\n\n    # Summary statistics\n    print(df_sampled.describe())\n    # Visualize the distribution of numerical columns\n    # Numerical columns for distribution\n    numerical_cols = ['sales', 'sell_price', 'wm_yr_wk', 'month', 'wday']\n\n    for col in numerical_cols:\n        plt.figure(figsize=(10, 6))\n        sns.histplot(df_sampled[col], kde=True, bins=20)\n        plt.title(f\"Distribution of {col} for {store}\")\n        plt.savefig(f'distribution_of_{col}_for_{store}.png', dpi=300)\n        plt.show()\n\n    # Categorical columns for distribution\n    categorical_cols = ['weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\n\n    for col in categorical_cols:\n        plt.figure(figsize=(10, 6))\n        sns.countplot(data=df_sampled, x=col)\n        plt.title(f\"Distribution of {col} for {store}\")\n        plt.xticks(rotation=45)\n        plt.savefig(f'distribution_of_{col}_for_{store}.png', dpi=300)\n        plt.show()\n\n    \n\n    # Encode categorical columns for correlation heatmap\n    df_encoded = df_sampled.copy()\n    le = LabelEncoder()\n    for col in categorical_cols:\n        df_encoded[col] = le.fit_transform(df_encoded[col])\n\n    # Compute correlation matrix for numerical columns\n    correlation_matrix = df_encoded[numerical_cols + categorical_cols].corr()\n\n    # Plot heatmap of the correlation matrix\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n    plt.title(f\"Correlation Matrix for {store}\")\n    plt.savefig(f'corr_matrix_for_{store}.png', dpi=300)\n    plt.show()\n\n    # Time series plot for 'sales'\n    plt.figure(figsize=(14, 7))\n    df_sampled['date'] = pd.to_datetime(df_sampled['date'])\n    df_sampled.groupby('date')['sales'].sum().plot()\n    plt.title(f\"Sales Time Series for {store}\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Total Sales\")\n    plt.savefig(f\"time_series_for_{store}.png\", dpi=300)\n    plt.show()\n\n    # Boxplot to detect outliers in sales data\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x=df_sampled['sales'])\n    plt.title(f\"Outliers in Sales for {store}\")\n    plt.savefig(f'outliers_in_sales_for_{store}.png', dpi=300)\n    plt.show()\n\n\n# Compare sales trends across stores using sampled data\nplt.figure(figsize=(14, 7))\nfor store in all_stores:\n    store_data = pd.read_pickle(f\"/kaggle/working/merged_{store}.pkl\")\n    \n    # Sample the data for faster processing\n    store_data_sampled = store_data.sample(frac=0.1, random_state=42)\n    \n    store_data_sampled['date'] = pd.to_datetime(store_data_sampled['date'])\n    store_data_sampled.groupby('date')['sales'].sum().plot(label=store)\n\nplt.legend()\nplt.title(\"Sales Trends Across Stores (Sampled Data)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Total Sales\")\nplt.savefig(f'sales_trends_across_stores_sampled.png', dpi=300)\nplt.show()\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}